# Story 6.4: Setup Alert Management

**Epic:** 6 - Monitoring & Observability  
**Story Number:** 6.4  
**Title:** Configure AlertManager and PagerDuty Integration  
**Status:** READY  
**Points:** 3  
**Component:** 923 (AlertManager)  

---

## Story

**As a** Operations Engineer,  
**I want** intelligent alert routing and escalation,  
**so that** the right teams are notified of issues with minimal alert fatigue.

---

## Acceptance Criteria

1. AlertManager deployed with HA configuration
2. Alert routing rules configured by severity and team
3. PagerDuty integration for critical alerts
4. Alert deduplication and grouping implemented
5. Silence management with audit trail
6. Escalation policies configured
7. Alert analytics and reporting enabled
8. Integration with Slack, Teams, and email

---

## Tasks / Subtasks

### Task 1: Write Alert Management Tests (TDD) (AC: 1, 2, 4)
- [ ] Create `monitoring/alertmanager_test.go`
  - [ ] Test AlertManager deployment
  - [ ] Test alert routing
  - [ ] Test deduplication
  - [ ] Test integrations
- [ ] Run tests to confirm they fail (Red phase)

### Task 2: Deploy AlertManager Infrastructure (AC: 1, 2)
- [ ] Deploy AlertManager with Helm
  ```yaml
  # alertmanager/values.yaml
  alertmanager:
    enabled: true
    
    config:
      global:
        resolve_timeout: 5m
        http_config:
          tls_config:
            insecure_skip_verify: false
        smtp_smarthost: 'smtp.sendgrid.net:587'
        smtp_from: 'alerts@oversight.com'
        smtp_auth_username: 'apikey'
        smtp_auth_password: '${SENDGRID_API_KEY}'
        smtp_require_tls: true
        
        # PagerDuty configuration
        pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
        
        # Slack configuration
        slack_api_url: '${SLACK_WEBHOOK_URL}'
      
      templates:
        - '/etc/alertmanager/templates/*.tmpl'
      
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 12h
        receiver: 'default'
        
        routes:
          # Critical alerts - immediate page
          - match:
              severity: critical
            receiver: 'pagerduty-critical'
            group_wait: 0s
            repeat_interval: 5m
            
          # High priority - notify on-call
          - match:
              severity: high
            receiver: 'pagerduty-high'
            group_wait: 30s
            repeat_interval: 1h
            
          # Security alerts
          - match:
              type: security
            receiver: 'security-team'
            group_by: ['alertname', 'severity']
            
          # Database alerts
          - match_re:
              service: ^(postgresql|redis|mongodb)
            receiver: 'database-team'
            
          # Application alerts
          - match:
              team: application
            receiver: 'app-team'
            
          # Infrastructure alerts
          - match:
              team: infrastructure
            receiver: 'infra-team'
      
      receivers:
        - name: 'default'
          slack_configs:
            - channel: '#alerts'
              title: 'Alert: {{ .GroupLabels.alertname }}'
              text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
              send_resolved: true
        
        - name: 'pagerduty-critical'
          pagerduty_configs:
            - service_key: '${PAGERDUTY_SERVICE_KEY_CRITICAL}'
              description: '{{ .GroupLabels.alertname }}'
              details:
                severity: '{{ .CommonLabels.severity }}'
                cluster: '{{ .CommonLabels.cluster }}'
                namespace: '{{ .CommonLabels.namespace }}'
                description: '{{ .CommonAnnotations.description }}'
                runbook: '{{ .CommonAnnotations.runbook_url }}'
          slack_configs:
            - channel: '#critical-alerts'
              title: 'CRITICAL: {{ .GroupLabels.alertname }}'
              color: 'danger'
        
        - name: 'pagerduty-high'
          pagerduty_configs:
            - service_key: '${PAGERDUTY_SERVICE_KEY_HIGH}'
              description: '{{ .GroupLabels.alertname }}'
          slack_configs:
            - channel: '#alerts'
              title: 'HIGH: {{ .GroupLabels.alertname }}'
              color: 'warning'
        
        - name: 'security-team'
          email_configs:
            - to: 'security@oversight.com'
              headers:
                Subject: 'Security Alert: {{ .GroupLabels.alertname }}'
          slack_configs:
            - channel: '#security-alerts'
              username: 'SecurityBot'
              icon_emoji: ':shield:'
          webhook_configs:
            - url: '${SECURITY_WEBHOOK_URL}'
        
        - name: 'database-team'
          email_configs:
            - to: 'database-team@oversight.com'
          slack_configs:
            - channel: '#database-alerts'
        
        - name: 'app-team'
          slack_configs:
            - channel: '#app-alerts'
          teams_configs:
            - webhook_url: '${TEAMS_WEBHOOK_URL}'
        
        - name: 'infra-team'
          slack_configs:
            - channel: '#infra-alerts'
      
      inhibit_rules:
        - source_match:
            severity: 'critical'
          target_match:
            severity: 'warning'
          equal: ['alertname', 'cluster', 'service']
        
        - source_match:
            alertname: 'NodeDown'
          target_match:
            alertname: 'TargetDown'
          equal: ['node']
  
  replicaCount: 3
  
  persistence:
    enabled: true
    size: 2Gi
    storageClass: managed-premium
  
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: alertmanager
          topologyKey: kubernetes.io/hostname
  ```
- [ ] Configure HA clustering
  ```yaml
  # alertmanager-ha.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: alertmanager-mesh
    namespace: monitoring
  spec:
    clusterIP: None
    ports:
      - name: mesh
        port: 9094
        protocol: TCP
        targetPort: 9094
    selector:
      app: alertmanager
  
  ---
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    name: alertmanager
    namespace: monitoring
  spec:
    replicas: 3
    serviceName: alertmanager-mesh
    template:
      spec:
        containers:
          - name: alertmanager
            args:
              - --config.file=/etc/alertmanager/alertmanager.yml
              - --storage.path=/alertmanager
              - --cluster.listen-address=0.0.0.0:9094
              - --cluster.peer=alertmanager-0.alertmanager-mesh:9094
              - --cluster.peer=alertmanager-1.alertmanager-mesh:9094
              - --cluster.peer=alertmanager-2.alertmanager-mesh:9094
              - --cluster.reconnect-timeout=5m
  ```
- [ ] Setup configuration reloading
- [ ] Configure storage backend

### Task 3: Configure Alert Routing (AC: 2, 6)
- [ ] Define routing rules
  ```yaml
  # alert-routing-rules.yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: alertmanager-routes
    namespace: monitoring
  data:
    routes.yaml: |
      # Routing decision tree
      routes:
        # Business hours vs after hours
        - match:
            severity: high
            time_of_day: business_hours
          receiver: slack
          continue: true
        
        - match:
            severity: high
            time_of_day: after_hours
          receiver: pagerduty
          continue: true
        
        # Service-specific routing
        - match_re:
            service: "(api|web|mobile)"
          receiver: app-team
          routes:
            - match:
                severity: critical
              receiver: app-team-oncall
        
        - match_re:
            service: "(database|cache|queue)"
          receiver: data-team
          routes:
            - match:
                severity: critical
              receiver: data-team-oncall
        
        # Region-specific routing
        - match:
            region: us-east-1
          receiver: us-east-team
        
        - match:
            region: eu-west-1
          receiver: eu-west-team
        
        # Customer-specific alerts
        - match:
            customer_tier: enterprise
          receiver: enterprise-support
          group_wait: 0s
          repeat_interval: 30m
  ```
- [ ] Create escalation policies
  ```javascript
  // services/escalation.js
  class EscalationService {
    constructor() {
      this.policies = {
        critical: {
          levels: [
            { delay: 0, targets: ['oncall-primary'] },
            { delay: 300, targets: ['oncall-secondary'] },
            { delay: 900, targets: ['oncall-manager'] },
            { delay: 1800, targets: ['oncall-director'] }
          ]
        },
        high: {
          levels: [
            { delay: 300, targets: ['oncall-primary'] },
            { delay: 1800, targets: ['oncall-secondary'] }
          ]
        },
        medium: {
          levels: [
            { delay: 1800, targets: ['team-slack'] }
          ]
        }
      };
    }
    
    async escalate(alert) {
      const policy = this.policies[alert.severity];
      const startTime = alert.created_at;
      
      for (const level of policy.levels) {
        const elapsed = Date.now() - startTime;
        
        if (elapsed >= level.delay * 1000 && !alert.acknowledged) {
          await this.notifyTargets(level.targets, alert);
          alert.escalation_level = policy.levels.indexOf(level) + 1;
        }
      }
    }
    
    async notifyTargets(targets, alert) {
      for (const target of targets) {
        if (target.startsWith('oncall-')) {
          await this.pageOncall(target, alert);
        } else if (target.includes('slack')) {
          await this.notifySlack(target, alert);
        } else if (target.includes('email')) {
          await this.sendEmail(target, alert);
        }
      }
    }
    
    async pageOncall(role, alert) {
      const oncallUser = await this.getOncallUser(role);
      
      await this.pagerduty.createIncident({
        title: alert.name,
        service_id: oncallUser.service_id,
        urgency: alert.severity === 'critical' ? 'high' : 'low',
        details: {
          alert_id: alert.id,
          description: alert.description,
          runbook: alert.runbook_url,
          metrics: alert.metrics_url,
          logs: alert.logs_url
        }
      });
    }
  }
  ```
- [ ] Setup on-call schedules
- [ ] Configure override rules

### Task 4: Integrate PagerDuty (AC: 3)
- [ ] Configure PagerDuty integration
  ```javascript
  // integrations/pagerduty.js
  const { PagerDutyClient } = require('@pagerduty/pdjs');
  
  class PagerDutyIntegration {
    constructor() {
      this.client = new PagerDutyClient({
        token: process.env.PAGERDUTY_API_TOKEN
      });
      
      this.services = {
        critical: process.env.PAGERDUTY_SERVICE_ID_CRITICAL,
        high: process.env.PAGERDUTY_SERVICE_ID_HIGH,
        database: process.env.PAGERDUTY_SERVICE_ID_DATABASE,
        security: process.env.PAGERDUTY_SERVICE_ID_SECURITY
      };
    }
    
    async createIncident(alert) {
      const incident = {
        incident: {
          type: 'incident',
          title: alert.summary,
          service: {
            id: this.getServiceId(alert),
            type: 'service_reference'
          },
          urgency: this.getUrgency(alert),
          body: {
            type: 'incident_body',
            details: this.formatDetails(alert)
          },
          incident_key: alert.fingerprint,
          escalation_policy: {
            id: this.getEscalationPolicy(alert),
            type: 'escalation_policy_reference'
          }
        }
      };
      
      try {
        const response = await this.client.post('/incidents', incident);
        
        // Store incident ID for tracking
        await this.storeIncidentMapping(alert.fingerprint, response.data.incident.id);
        
        return response.data.incident;
      } catch (error) {
        console.error('Failed to create PagerDuty incident:', error);
        throw error;
      }
    }
    
    async resolveIncident(alertFingerprint) {
      const incidentId = await this.getIncidentId(alertFingerprint);
      
      if (!incidentId) return;
      
      await this.client.put(`/incidents/${incidentId}`, {
        incident: {
          type: 'incident',
          status: 'resolved'
        }
      });
    }
    
    async acknowledgeIncident(alertFingerprint, userId) {
      const incidentId = await this.getIncidentId(alertFingerprint);
      
      if (!incidentId) return;
      
      await this.client.put(`/incidents/${incidentId}`, {
        incident: {
          type: 'incident',
          status: 'acknowledged',
          acknowledgements: [{
            acknowledger: {
              id: userId,
              type: 'user_reference'
            }
          }]
        }
      });
    }
    
    formatDetails(alert) {
      return `
        Alert: ${alert.name}
        Severity: ${alert.severity}
        Environment: ${alert.labels.environment}
        Service: ${alert.labels.service}
        
        Description: ${alert.annotations.description}
        
        Current Value: ${alert.value}
        Threshold: ${alert.threshold}
        
        Runbook: ${alert.annotations.runbook_url}
        Dashboard: ${alert.annotations.dashboard_url}
        Logs: ${alert.annotations.logs_url}
        
        Labels: ${JSON.stringify(alert.labels, null, 2)}
      `;
    }
    
    getServiceId(alert) {
      if (alert.labels.team === 'security') return this.services.security;
      if (alert.labels.service?.includes('database')) return this.services.database;
      if (alert.severity === 'critical') return this.services.critical;
      return this.services.high;
    }
    
    getUrgency(alert) {
      return alert.severity === 'critical' ? 'high' : 'low';
    }
  }
  ```
- [ ] Setup on-call schedules
- [ ] Configure escalation policies
- [ ] Test incident creation

### Task 5: Implement Alert Deduplication (AC: 4)
- [ ] Configure deduplication rules
  ```yaml
  # deduplication-config.yaml
  deduplication:
    # Group similar alerts
    grouping:
      - by: [alertname, environment, service]
        window: 5m
      
      - by: [alertname, node]
        window: 10m
      
      - by: [alertname, pod]
        window: 2m
    
    # Fingerprinting for dedup
    fingerprint:
      - alertname
      - environment  
      - service
      - severity
    
    # Smart grouping rules
    smart_grouping:
      enabled: true
      rules:
        - name: "Pod restart storm"
          condition: |
            alertname == "PodRestartingTooOften" && 
            count() > 10
          group_as: "Multiple Pods Restarting"
        
        - name: "Node issues"
          condition: |
            alertname =~ "Node.*" &&
            same_node == true
          group_as: "Node {{.node}} Issues"
        
        - name: "Database connection surge"
          condition: |
            alertname == "DatabaseConnectionFailed" &&
            time_window("1m") &&
            count() > 5
          group_as: "Database Connection Issues"
  ```
- [ ] Implement fingerprinting
  ```javascript
  // services/alert-dedup.js
  const crypto = require('crypto');
  
  class AlertDeduplication {
    constructor() {
      this.activeAlerts = new Map();
      this.groupingWindows = new Map();
    }
    
    generateFingerprint(alert) {
      const parts = [
        alert.labels.alertname,
        alert.labels.environment || 'default',
        alert.labels.service || '',
        alert.labels.namespace || '',
        alert.labels.pod ? alert.labels.pod.split('-').slice(0, -2).join('-') : ''
      ];
      
      return crypto
        .createHash('sha256')
        .update(parts.join(':'))
        .digest('hex');
    }
    
    isDuplicate(alert) {
      const fingerprint = this.generateFingerprint(alert);
      const existing = this.activeAlerts.get(fingerprint);
      
      if (!existing) {
        this.activeAlerts.set(fingerprint, {
          alert,
          count: 1,
          firstSeen: Date.now(),
          lastSeen: Date.now()
        });
        return false;
      }
      
      // Update existing alert
      existing.count++;
      existing.lastSeen = Date.now();
      existing.alert = alert; // Update with latest data
      
      // Check if we should re-alert based on time
      const timeSinceFirst = Date.now() - existing.firstSeen;
      const repeatInterval = this.getRepeatInterval(alert);
      
      if (timeSinceFirst > repeatInterval) {
        existing.firstSeen = Date.now();
        return false; // Allow re-alerting
      }
      
      return true;
    }
    
    getRepeatInterval(alert) {
      const intervals = {
        critical: 5 * 60 * 1000,    // 5 minutes
        high: 30 * 60 * 1000,        // 30 minutes
        medium: 2 * 60 * 60 * 1000,  // 2 hours
        low: 12 * 60 * 60 * 1000     // 12 hours
      };
      
      return intervals[alert.labels.severity] || intervals.medium;
    }
    
    groupAlerts(alerts) {
      const groups = new Map();
      
      for (const alert of alerts) {
        const groupKey = this.getGroupKey(alert);
        
        if (!groups.has(groupKey)) {
          groups.set(groupKey, {
            key: groupKey,
            alerts: [],
            summary: this.generateGroupSummary(groupKey),
            severity: alert.labels.severity
          });
        }
        
        const group = groups.get(groupKey);
        group.alerts.push(alert);
        
        // Update group severity to highest
        if (this.severityValue(alert.labels.severity) > 
            this.severityValue(group.severity)) {
          group.severity = alert.labels.severity;
        }
      }
      
      return Array.from(groups.values());
    }
    
    getGroupKey(alert) {
      // Group by service and alert type
      return `${alert.labels.service || 'unknown'}-${alert.labels.alertname}`;
    }
    
    severityValue(severity) {
      const values = { critical: 4, high: 3, medium: 2, low: 1 };
      return values[severity] || 0;
    }
  }
  ```
- [ ] Setup correlation rules
- [ ] Test deduplication logic

### Task 6: Implement Silence Management (AC: 5)
- [ ] Create silence API
  ```javascript
  // api/silences.js
  const express = require('express');
  const router = express.Router();
  
  class SilenceManager {
    constructor(alertmanager) {
      this.alertmanager = alertmanager;
      this.auditLog = [];
    }
    
    async createSilence(req, res) {
      const { matchers, duration, comment, createdBy } = req.body;
      
      // Validate request
      if (!matchers || !duration || !comment) {
        return res.status(400).json({ error: 'Missing required fields' });
      }
      
      const silence = {
        id: this.generateId(),
        matchers: matchers.map(m => ({
          name: m.name,
          value: m.value,
          isRegex: m.isRegex || false
        })),
        startsAt: new Date().toISOString(),
        endsAt: new Date(Date.now() + duration * 1000).toISOString(),
        createdBy,
        comment,
        status: {
          state: 'active'
        }
      };
      
      try {
        // Create silence in AlertManager
        const response = await this.alertmanager.createSilence(silence);
        
        // Audit log
        this.auditLog.push({
          action: 'create',
          silence: response.data.silenceID,
          user: createdBy,
          timestamp: new Date(),
          matchers,
          duration,
          comment
        });
        
        // Send notification
        await this.notifySilenceCreated(silence);
        
        res.json({ silenceId: response.data.silenceID });
      } catch (error) {
        console.error('Failed to create silence:', error);
        res.status(500).json({ error: 'Failed to create silence' });
      }
    }
    
    async updateSilence(req, res) {
      const { id } = req.params;
      const { duration, comment, updatedBy } = req.body;
      
      try {
        const silence = await this.alertmanager.getSilence(id);
        
        if (!silence) {
          return res.status(404).json({ error: 'Silence not found' });
        }
        
        // Update silence
        silence.endsAt = new Date(Date.now() + duration * 1000).toISOString();
        silence.comment = `${silence.comment}\nUPDATED: ${comment}`;
        
        await this.alertmanager.updateSilence(id, silence);
        
        // Audit log
        this.auditLog.push({
          action: 'update',
          silence: id,
          user: updatedBy,
          timestamp: new Date(),
          changes: { duration, comment }
        });
        
        res.json({ success: true });
      } catch (error) {
        console.error('Failed to update silence:', error);
        res.status(500).json({ error: 'Failed to update silence' });
      }
    }
    
    async deleteSilence(req, res) {
      const { id } = req.params;
      const { deletedBy, reason } = req.body;
      
      try {
        await this.alertmanager.deleteSilence(id);
        
        // Audit log
        this.auditLog.push({
          action: 'delete',
          silence: id,
          user: deletedBy,
          timestamp: new Date(),
          reason
        });
        
        // Notify about deletion
        await this.notifySilenceDeleted(id, deletedBy, reason);
        
        res.json({ success: true });
      } catch (error) {
        console.error('Failed to delete silence:', error);
        res.status(500).json({ error: 'Failed to delete silence' });
      }
    }
    
    async getAuditLog(req, res) {
      const { startTime, endTime, user, action } = req.query;
      
      let logs = [...this.auditLog];
      
      // Filter by time range
      if (startTime) {
        logs = logs.filter(l => l.timestamp >= new Date(startTime));
      }
      if (endTime) {
        logs = logs.filter(l => l.timestamp <= new Date(endTime));
      }
      
      // Filter by user
      if (user) {
        logs = logs.filter(l => l.user === user);
      }
      
      // Filter by action
      if (action) {
        logs = logs.filter(l => l.action === action);
      }
      
      res.json(logs);
    }
    
    generateId() {
      return `sil-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    }
  }
  ```
- [ ] Create silence UI
- [ ] Setup audit logging
- [ ] Configure auto-expiry

### Task 7: Enable Alert Analytics (AC: 7)
- [ ] Create analytics dashboard
  ```sql
  -- analytics/alert-metrics.sql
  -- Alert frequency analysis
  CREATE VIEW alert_frequency AS
  SELECT 
    alertname,
    severity,
    DATE_TRUNC('hour', fired_at) as hour,
    COUNT(*) as alert_count,
    AVG(resolved_at - fired_at) as avg_resolution_time
  FROM alerts
  WHERE fired_at > NOW() - INTERVAL '30 days'
  GROUP BY alertname, severity, hour;
  
  -- Top alerting services
  CREATE VIEW top_alerting_services AS
  SELECT 
    service,
    COUNT(*) as total_alerts,
    COUNT(DISTINCT alertname) as unique_alerts,
    AVG(CASE WHEN resolved_at IS NOT NULL 
         THEN EXTRACT(EPOCH FROM (resolved_at - fired_at))/60 
         END) as avg_resolution_minutes
  FROM alerts
  WHERE fired_at > NOW() - INTERVAL '7 days'
  GROUP BY service
  ORDER BY total_alerts DESC
  LIMIT 20;
  
  -- Alert noise analysis
  CREATE VIEW alert_noise AS
  SELECT 
    alertname,
    COUNT(*) as fire_count,
    COUNT(DISTINCT DATE_TRUNC('day', fired_at)) as days_with_alert,
    CASE 
      WHEN COUNT(*) > 100 AND AVG(resolved_at - fired_at) < INTERVAL '5 minutes' 
      THEN 'HIGH_NOISE'
      WHEN COUNT(*) > 50 
      THEN 'MODERATE_NOISE'
      ELSE 'LOW_NOISE'
    END as noise_level
  FROM alerts
  WHERE fired_at > NOW() - INTERVAL '30 days'
  GROUP BY alertname;
  
  -- MTTR by team
  CREATE VIEW mttr_by_team AS
  SELECT 
    team,
    severity,
    PERCENTILE_CONT(0.5) WITHIN GROUP (
      ORDER BY EXTRACT(EPOCH FROM (acknowledged_at - fired_at))/60
    ) as median_tta_minutes,
    PERCENTILE_CONT(0.5) WITHIN GROUP (
      ORDER BY EXTRACT(EPOCH FROM (resolved_at - fired_at))/60
    ) as median_ttr_minutes,
    COUNT(*) as alert_count
  FROM alerts
  WHERE resolved_at IS NOT NULL
    AND fired_at > NOW() - INTERVAL '30 days'
  GROUP BY team, severity;
  ```
- [ ] Create reporting service
  ```javascript
  // services/alert-analytics.js
  class AlertAnalytics {
    constructor(database) {
      this.db = database;
    }
    
    async generateWeeklyReport() {
      const report = {
        generatedAt: new Date(),
        period: {
          start: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000),
          end: new Date()
        },
        metrics: {}
      };
      
      // Total alerts
      report.metrics.totalAlerts = await this.db.query(`
        SELECT COUNT(*) as count, severity
        FROM alerts
        WHERE fired_at > NOW() - INTERVAL '7 days'
        GROUP BY severity
      `);
      
      // Resolution metrics
      report.metrics.resolution = await this.db.query(`
        SELECT 
          AVG(EXTRACT(EPOCH FROM (resolved_at - fired_at))/60) as avg_minutes,
          PERCENTILE_CONT(0.5) WITHIN GROUP (
            ORDER BY EXTRACT(EPOCH FROM (resolved_at - fired_at))/60
          ) as median_minutes,
          PERCENTILE_CONT(0.95) WITHIN GROUP (
            ORDER BY EXTRACT(EPOCH FROM (resolved_at - fired_at))/60
          ) as p95_minutes
        FROM alerts
        WHERE resolved_at IS NOT NULL
          AND fired_at > NOW() - INTERVAL '7 days'
      `);
      
      // Top alerts
      report.metrics.topAlerts = await this.db.query(`
        SELECT alertname, COUNT(*) as count
        FROM alerts
        WHERE fired_at > NOW() - INTERVAL '7 days'
        GROUP BY alertname
        ORDER BY count DESC
        LIMIT 10
      `);
      
      // Noise analysis
      report.metrics.noisyAlerts = await this.db.query(`
        SELECT * FROM alert_noise
        WHERE noise_level IN ('HIGH_NOISE', 'MODERATE_NOISE')
        ORDER BY fire_count DESC
      `);
      
      // Team performance
      report.metrics.teamPerformance = await this.db.query(`
        SELECT * FROM mttr_by_team
        ORDER BY median_ttr_minutes
      `);
      
      return report;
    }
    
    async analyzeAlertPatterns() {
      // Time-based patterns
      const timePatterns = await this.db.query(`
        SELECT 
          EXTRACT(HOUR FROM fired_at) as hour,
          EXTRACT(DOW FROM fired_at) as day_of_week,
          COUNT(*) as alert_count
        FROM alerts
        WHERE fired_at > NOW() - INTERVAL '30 days'
        GROUP BY hour, day_of_week
        ORDER BY alert_count DESC
      `);
      
      // Correlation analysis
      const correlations = await this.db.query(`
        SELECT 
          a1.alertname as alert1,
          a2.alertname as alert2,
          COUNT(*) as co_occurrence
        FROM alerts a1
        JOIN alerts a2 ON 
          a1.service = a2.service AND
          ABS(EXTRACT(EPOCH FROM (a1.fired_at - a2.fired_at))) < 300 AND
          a1.alertname < a2.alertname
        WHERE a1.fired_at > NOW() - INTERVAL '7 days'
        GROUP BY alert1, alert2
        HAVING COUNT(*) > 5
        ORDER BY co_occurrence DESC
      `);
      
      return { timePatterns, correlations };
    }
  }
  ```
- [ ] Setup alert optimization
- [ ] Create noise reduction rules

### Task 8: Configure Communication Channels (AC: 8)
- [ ] Setup Slack integration
  ```javascript
  // integrations/slack.js
  const { WebClient } = require('@slack/web-api');
  
  class SlackIntegration {
    constructor() {
      this.client = new WebClient(process.env.SLACK_TOKEN);
      this.channels = {
        critical: '#critical-alerts',
        high: '#alerts',
        medium: '#alerts',
        low: '#alerts-low',
        security: '#security-alerts',
        database: '#database-alerts'
      };
    }
    
    async sendAlert(alert) {
      const channel = this.getChannel(alert);
      const message = this.formatMessage(alert);
      
      try {
        const result = await this.client.chat.postMessage({
          channel,
          ...message
        });
        
        // Store message timestamp for updates
        await this.storeMessageId(alert.fingerprint, result.ts, channel);
        
        return result;
      } catch (error) {
        console.error('Failed to send Slack message:', error);
      }
    }
    
    formatMessage(alert) {
      const color = this.getSeverityColor(alert.labels.severity);
      
      return {
        text: `Alert: ${alert.labels.alertname}`,
        attachments: [{
          color,
          title: alert.labels.alertname,
          text: alert.annotations.description,
          fields: [
            {
              title: 'Severity',
              value: alert.labels.severity.toUpperCase(),
              short: true
            },
            {
              title: 'Service',
              value: alert.labels.service || 'N/A',
              short: true
            },
            {
              title: 'Environment',
              value: alert.labels.environment || 'N/A',
              short: true
            },
            {
              title: 'Value',
              value: alert.value || 'N/A',
              short: true
            }
          ],
          actions: [
            {
              type: 'button',
              text: 'View Dashboard',
              url: alert.annotations.dashboard_url
            },
            {
              type: 'button',
              text: 'View Logs',
              url: alert.annotations.logs_url
            },
            {
              type: 'button',
              text: 'Runbook',
              url: alert.annotations.runbook_url
            },
            {
              type: 'button',
              text: 'Acknowledge',
              style: 'primary',
              action_id: 'acknowledge_alert',
              value: alert.fingerprint
            }
          ],
          footer: 'AlertManager',
          ts: Math.floor(Date.now() / 1000)
        }]
      };
    }
  }
  ```
- [ ] Setup Teams integration
- [ ] Configure email templates
- [ ] Test all channels

---

## Dev Notes

### AlertManager URLs
```
API: http://alertmanager:9093/api/v1
UI: https://alerts.oversight.com
Mesh: alertmanager-0:9094
```

### Alert Routing Logic
```
Priority Order:
1. Security alerts → security-team
2. Critical severity → pagerduty-critical
3. High severity → pagerduty-high  
4. Service-specific → respective teams
5. Default → slack #alerts
```

### Silence Guidelines
- Max duration: 24 hours (extend if needed)
- Require comment for all silences
- Auto-expire after duration
- Audit all silence operations
- Notify team when silencing

### Testing Standards
- Test location: `/tests/monitoring/alerts/`
- Test routing logic
- Verify deduplication
- Test all integrations
- Monitor alert volume

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-22 | 1.0 | Initial story creation | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated during development_

### Completion Notes List
_To be populated during development_

### File List
_To be populated during development_

---

## QA Results
_To be populated by QA agent_