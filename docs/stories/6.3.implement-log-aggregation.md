# Story 6.3: Implement Log Aggregation

**Epic:** 6 - Monitoring & Observability  
**Story Number:** 6.3  
**Title:** Deploy Loki for Centralized Log Management  
**Status:** READY  
**Points:** 5  
**Component:** 922 (Loki)  

---

## Story

**As a** DevOps Engineer,  
**I want** centralized log aggregation and querying,  
**so that** we can troubleshoot issues across all services from a single interface.

---

## Acceptance Criteria

1. Loki deployed with distributed architecture
2. Log ingestion from all components configured
3. Log retention policies implemented (30 days hot, 90 days cold)
4. Promtail agents deployed on all nodes
5. LogQL queries optimized for performance
6. Log correlation with traces and metrics
7. Security logs segregated with access control
8. Real-time log streaming enabled

---

## Tasks / Subtasks

### Task 1: Write Log Aggregation Tests (TDD) (AC: 1, 4, 6)
- [ ] Create `monitoring/loki_test.go`
  - [ ] Test Loki deployment
  - [ ] Test log ingestion
  - [ ] Test query performance
  - [ ] Test retention policies
- [ ] Run tests to confirm they fail (Red phase)

### Task 2: Deploy Loki Infrastructure (AC: 1, 3)
- [ ] Deploy Loki with Helm
  ```yaml
  # loki/values.yaml
  loki:
    auth_enabled: false
    
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
      log_level: info
    
    distributor:
      ring:
        kvstore:
          store: consul
          consul:
            host: consul:8500
    
    ingester:
      lifecycler:
        ring:
          kvstore:
            store: consul
            consul:
              host: consul:8500
          replication_factor: 3
      chunk_idle_period: 30m
      chunk_retain_period: 5m
      max_transfer_retries: 0
      wal:
        enabled: true
        dir: /loki/wal
    
    schema_config:
      configs:
        - from: 2024-01-01
          store: boltdb-shipper
          object_store: azure
          schema: v12
          index:
            prefix: loki_index_
            period: 24h
    
    storage_config:
      azure:
        container_name: loki-chunks
        account_name: ${STORAGE_ACCOUNT}
        account_key: ${STORAGE_KEY}
        request_timeout: 0
      
      boltdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/cache
        shared_store: azure
        cache_ttl: 24h
      
      filesystem:
        directory: /loki/chunks
    
    compactor:
      working_directory: /loki/compactor
      shared_store: azure
      retention_enabled: true
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
    
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      ingestion_rate_mb: 100
      ingestion_burst_size_mb: 200
      per_stream_rate_limit: 10MB
      per_stream_rate_limit_burst: 20MB
      max_query_parallelism: 32
      max_cache_freshness_per_query: 10m
      retention_period: 744h  # 31 days
    
    query_range:
      align_queries_with_step: true
      max_retries: 5
      parallelise_shardable_queries: true
      cache_results: true
      results_cache:
        cache:
          enable_fifocache: true
          fifocache:
            max_size_items: 1024
            validity: 24h
    
    frontend:
      compress_responses: true
      log_queries_longer_than: 5s
      downstream_url: http://loki-querier:3100
    
    ruler:
      storage:
        type: azure
        azure:
          container_name: loki-ruler
      rule_path: /loki/rules
      alertmanager_url: http://alertmanager:9093
      ring:
        kvstore:
          store: consul
      enable_api: true
  
  # Deployment configuration
  gateway:
    enabled: true
    replicas: 3
    ingress:
      enabled: true
      hosts:
        - host: logs.oversight.com
          paths:
            - path: /
              pathType: Prefix
      tls:
        - secretName: loki-tls
          hosts:
            - logs.oversight.com
  
  distributor:
    replicas: 3
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
  
  ingester:
    replicas: 3
    persistence:
      enabled: true
      size: 10Gi
      storageClass: managed-premium
    resources:
      limits:
        cpu: 2
        memory: 4Gi
      requests:
        cpu: 1
        memory: 2Gi
  
  querier:
    replicas: 3
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
  
  query-frontend:
    replicas: 2
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi
  
  compactor:
    enabled: true
    persistence:
      enabled: true
      size: 10Gi
      storageClass: managed-premium
  ```
- [ ] Configure storage backend
  ```hcl
  resource "azurerm_storage_container" "loki_chunks" {
    name                  = "loki-chunks"
    storage_account_name  = azurerm_storage_account.monitoring.name
    container_access_type = "private"
  }
  
  resource "azurerm_storage_container" "loki_ruler" {
    name                  = "loki-ruler"
    storage_account_name  = azurerm_storage_account.monitoring.name
    container_access_type = "private"
  }
  
  resource "azurerm_storage_lifecycle_management_policy" "loki" {
    storage_account_id = azurerm_storage_account.monitoring.id
    
    rule {
      name    = "loki-chunks-lifecycle"
      enabled = true
      
      filters {
        prefix_match = ["loki-chunks/"]
        blob_types   = ["blockBlob"]
      }
      
      actions {
        base_blob {
          tier_to_cool_after_days_since_modification_greater_than = 30
          tier_to_archive_after_days_since_modification_greater_than = 90
          delete_after_days_since_modification_greater_than = 180
        }
      }
    }
  }
  ```
- [ ] Setup retention policies
- [ ] Configure compaction

### Task 3: Deploy Promtail Agents (AC: 4)
- [ ] Deploy Promtail DaemonSet
  ```yaml
  # promtail/values.yaml
  config:
    lokiAddress: http://loki-gateway:3100/loki/api/v1/push
    
    snippets:
      pipelineStages:
        - docker: {}
        - drop:
            expression: ".*health.*|.*ping.*|.*metrics.*"
        - regex:
            expression: '^(?P<time>[^ ]+) (?P<level>[^ ]+) (?P<message>.*)$'
        - labels:
            level:
        - timestamp:
            source: time
            format: RFC3339
        - multiline:
            firstline: '^\d{4}-\d{2}-\d{2}'
            max_wait_time: 3s
        - tenant:
            source: namespace
    
    scrapeConfigs:
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        pipeline_stages:
          - cri: {}
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_name]
            target_label: app
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_container_name]
            target_label: container
          - replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
            target_label: __path__
      
      - job_name: systemd-journal
        journal:
          path: /var/log/journal
          labels:
            job: systemd-journal
        relabel_configs:
          - source_labels: ['__journal__systemd_unit']
            target_label: 'unit'
          - source_labels: ['__journal__hostname']
            target_label: 'hostname'
      
      - job_name: syslog
        syslog:
          listen_address: 0.0.0.0:514
          listen_protocol: tcp
          label_structured_data: true
          labels:
            job: "syslog"
        relabel_configs:
          - source_labels: ['__syslog_message_hostname']
            target_label: 'host'
          - source_labels: ['__syslog_message_app_name']
            target_label: 'app'
      
      - job_name: azure-logs
        azure_event_hubs:
          event_hub_name: oversight-logs
          connection_string: ${EVENTHUB_CONNECTION_STRING}
          labels:
            job: azure
            source: eventhub
  
  daemonset:
    enabled: true
    
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  
  serviceMonitor:
    enabled: true
  
  tolerations:
    - effect: NoSchedule
      operator: Exists
  
  volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: varlibdocker
      hostPath:
        path: /var/lib/docker/containers
  
  volumeMounts:
    - name: varlog
      mountPath: /var/log
      readOnly: true
    - name: varlibdocker
      mountPath: /var/lib/docker/containers
      readOnly: true
  ```
- [ ] Configure log pipelines
  ```yaml
  # promtail-config.yaml
  server:
    http_listen_port: 3101
    grpc_listen_port: 0
  
  positions:
    filename: /tmp/positions.yaml
  
  clients:
    - url: http://loki-gateway:3100/loki/api/v1/push
      tenant_id: ${TENANT_ID}
      batchwait: 1s
      batchsize: 1048576
      basic_auth:
        username: ${LOKI_USER}
        password: ${LOKI_PASSWORD}
      tls_config:
        insecure_skip_verify: false
      backoff_config:
        min_period: 500ms
        max_period: 5m
        max_retries: 10
  
  scrape_configs:
    - job_name: application
      static_configs:
        - targets:
            - localhost
          labels:
            job: oversight-app
            __path__: /app/logs/*.log
      pipeline_stages:
        - json:
            expressions:
              level: level
              msg: message
              timestamp: timestamp
              trace_id: trace_id
              span_id: span_id
        - labels:
            level:
            trace_id:
        - timestamp:
            source: timestamp
            format: RFC3339
        - output:
            source: msg
  ```
- [ ] Setup log enrichment
- [ ] Configure buffering

### Task 4: Setup Log Ingestion (AC: 2)
- [ ] Configure application logging
  ```javascript
  // lib/logger.js
  const winston = require('winston');
  const LokiTransport = require('winston-loki');
  
  class Logger {
    constructor(service) {
      this.service = service;
      this.logger = winston.createLogger({
        level: process.env.LOG_LEVEL || 'info',
        format: winston.format.combine(
          winston.format.timestamp(),
          winston.format.errors({ stack: true }),
          winston.format.json()
        ),
        defaultMeta: {
          service,
          environment: process.env.NODE_ENV,
          version: process.env.APP_VERSION,
          host: require('os').hostname()
        },
        transports: [
          new winston.transports.Console({
            format: winston.format.combine(
              winston.format.colorize(),
              winston.format.simple()
            )
          }),
          new LokiTransport({
            host: process.env.LOKI_HOST || 'http://loki-gateway:3100',
            labels: { service, environment: process.env.NODE_ENV },
            json: true,
            format: winston.format.json(),
            replaceTimestamp: true,
            onConnectionError: (err) => console.error('Loki connection error:', err)
          })
        ]
      });
    }
    
    // Add trace context
    withTrace(traceId, spanId) {
      return {
        info: (msg, meta = {}) => this.logger.info(msg, { ...meta, trace_id: traceId, span_id: spanId }),
        error: (msg, meta = {}) => this.logger.error(msg, { ...meta, trace_id: traceId, span_id: spanId }),
        warn: (msg, meta = {}) => this.logger.warn(msg, { ...meta, trace_id: traceId, span_id: spanId }),
        debug: (msg, meta = {}) => this.logger.debug(msg, { ...meta, trace_id: traceId, span_id: spanId })
      };
    }
    
    // Structured logging
    logRequest(req, res, duration) {
      this.logger.info('HTTP Request', {
        method: req.method,
        path: req.path,
        status: res.statusCode,
        duration,
        ip: req.ip,
        userAgent: req.get('user-agent'),
        userId: req.user?.id
      });
    }
    
    logDatabaseQuery(query, params, duration) {
      this.logger.debug('Database Query', {
        query: query.substring(0, 1000),
        paramCount: params?.length,
        duration,
        slow: duration > 1000
      });
    }
    
    logError(error, context = {}) {
      this.logger.error('Application Error', {
        message: error.message,
        stack: error.stack,
        code: error.code,
        ...context
      });
    }
  }
  
  module.exports = Logger;
  ```
- [ ] Configure infrastructure logs
  ```yaml
  # fluent-bit/config.yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: fluent-bit-config
    namespace: monitoring
  data:
    fluent-bit.conf: |
      [SERVICE]
          Daemon Off
          Flush 1
          Log_Level info
          Parsers_File parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port 2020
          Health_Check On
      
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          Parser cri
          Tag kube.*
          Refresh_Interval 5
          Mem_Buf_Limit 5MB
          Skip_Long_Lines On
          Ignore_Older 1d
      
      [INPUT]
          Name systemd
          Tag host.*
          Systemd_Filter _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail On
      
      [FILTER]
          Name kubernetes
          Match kube.*
          Kube_URL https://kubernetes.default.svc:443
          Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token
          Kube_Tag_Prefix kube.var.log.containers.
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
      
      [FILTER]
          Name modify
          Match *
          Add cluster ${CLUSTER_NAME}
          Add region ${REGION}
      
      [OUTPUT]
          Name loki
          Match kube.*
          host loki-gateway
          port 3100
          labels job=fluent-bit, cluster=${CLUSTER_NAME}
          auto_kubernetes_labels true
          line_format json
      
      [OUTPUT]
          Name azure_blob
          Match host.*
          account_name ${STORAGE_ACCOUNT}
          shared_key ${STORAGE_KEY}
          container_name system-logs
          blob_type blockblob
          auto_create_container on
          path logs/%Y/%m/%d
  ```
- [ ] Setup log parsing
- [ ] Configure log forwarding

### Task 5: Optimize Query Performance (AC: 5)
- [ ] Create LogQL queries
  ```javascript
  // queries/logql-library.js
  const logQueries = {
    // Error analysis
    errorsLast1h: `
      {job="oversight-app"} |= "error" 
      | json 
      | level="error" 
      | line_format "{{.timestamp}} {{.service}} {{.message}}"
    `,
    
    // Request tracing
    slowRequests: `
      {job="oversight-app"} 
      | json 
      | duration > 1000 
      | line_format "{{.method}} {{.path}} {{.duration}}ms"
    `,
    
    // Security events
    failedLogins: `
      {job="oversight-app"} 
      |= "login failed" 
      | json 
      | line_format "{{.timestamp}} {{.ip}} {{.username}}"
    `,
    
    // Performance metrics
    requestRate: `
      rate({job="oversight-app"} |= "HTTP Request" [5m])
    `,
    
    // Error rate
    errorRate: `
      sum(rate({job="oversight-app"} |= "error" [5m])) 
      / 
      sum(rate({job="oversight-app"} [5m]))
    `,
    
    // Database queries
    slowQueries: `
      {job="oversight-app"} 
      |= "Database Query" 
      | json 
      | slow="true" 
      | line_format "{{.query}} {{.duration}}ms"
    `,
    
    // Container logs
    containerErrors: `
      {container="oversight-api"} 
      |~ "ERROR|FATAL|PANIC" 
      | json 
      | line_format "{{.level}} {{.msg}}"
    `,
    
    // Audit logs
    auditEvents: `
      {job="audit"} 
      | json 
      | action=~"CREATE|UPDATE|DELETE" 
      | line_format "{{.user}} {{.action}} {{.resource}}"
    `
  };
  
  class LogQLQueryBuilder {
    constructor() {
      this.query = '';
    }
    
    stream(selector) {
      this.query = `{${selector}}`;
      return this;
    }
    
    filter(expression) {
      this.query += ` |= "${expression}"`;
      return this;
    }
    
    regex(pattern) {
      this.query += ` |~ "${pattern}"`;
      return this;
    }
    
    json() {
      this.query += ' | json';
      return this;
    }
    
    logfmt() {
      this.query += ' | logfmt';
      return this;
    }
    
    pattern(p) {
      this.query += ` | pattern \`${p}\``;
      return this;
    }
    
    lineFormat(format) {
      this.query += ` | line_format "${format}"`;
      return this;
    }
    
    unwrap(field) {
      this.query += ` | unwrap ${field}`;
      return this;
    }
    
    rate(duration) {
      this.query = `rate(${this.query} [${duration}])`;
      return this;
    }
    
    sum() {
      this.query = `sum(${this.query})`;
      return this;
    }
    
    build() {
      return this.query;
    }
  }
  ```
- [ ] Create query indexes
- [ ] Setup query caching
- [ ] Optimize retention

### Task 6: Implement Log Correlation (AC: 6)
- [ ] Setup trace correlation
  ```javascript
  // middleware/correlation.js
  const { v4: uuidv4 } = require('uuid');
  const opentelemetry = require('@opentelemetry/api');
  
  class CorrelationMiddleware {
    constructor(logger) {
      this.logger = logger;
      this.tracer = opentelemetry.trace.getTracer('oversight-app');
    }
    
    middleware() {
      return (req, res, next) => {
        // Get or create trace ID
        const traceId = req.headers['x-trace-id'] || uuidv4();
        const spanId = uuidv4();
        
        // Create span
        const span = this.tracer.startSpan(req.path, {
          attributes: {
            'http.method': req.method,
            'http.url': req.url,
            'http.target': req.path,
            'user.id': req.user?.id
          }
        });
        
        // Attach to request
        req.traceId = traceId;
        req.spanId = spanId;
        req.span = span;
        
        // Create correlated logger
        req.logger = this.logger.withTrace(traceId, spanId);
        
        // Log request
        req.logger.info('Request started', {
          method: req.method,
          path: req.path,
          query: req.query
        });
        
        // Propagate headers
        res.setHeader('X-Trace-Id', traceId);
        res.setHeader('X-Span-Id', spanId);
        
        // Capture response
        const originalSend = res.send;
        res.send = function(data) {
          span.setStatus({
            code: res.statusCode < 400 ? 0 : 1
          });
          span.end();
          
          req.logger.info('Request completed', {
            status: res.statusCode,
            duration: Date.now() - req.startTime
          });
          
          originalSend.call(this, data);
        };
        
        req.startTime = Date.now();
        next();
      };
    }
  }
  ```
- [ ] Link logs to metrics
  ```yaml
  # grafana/datasource-links.yaml
  datasources:
    - name: Loki-to-Prometheus
      type: loki
      jsonData:
        derivedFields:
          - datasourceUid: prometheus-uid
            matcherRegex: "trace_id=(\\w+)"
            name: TraceID
            url: "/explore?left=[\"now-1h\",\"now\",\"Tempo\",{\"query\":\"${__value.raw}\"}]"
          
          - datasourceUid: prometheus-uid
            matcherRegex: "metric_name=(\\w+)"
            name: View Metric
            url: "/explore?left=[\"now-1h\",\"now\",\"Prometheus\",{\"expr\":\"${__value.raw}\"}]"
    
    - name: Prometheus-to-Loki
      type: prometheus
      jsonData:
        customQueryParameters: "var-namespace=${__field.labels.namespace}&var-pod=${__field.labels.pod}"
        derivedFields:
          - datasourceUid: loki-uid
            matcherRegex: ".*"
            name: View Logs
            url: "/explore?left=[\"now-1h\",\"now\",\"Loki\",{\"expr\":\"{namespace=\\\"${__field.labels.namespace}\\\",pod=\\\"${__field.labels.pod}\\\"}\"}]"
  ```
- [ ] Setup distributed tracing
- [ ] Create correlation dashboards

### Task 7: Configure Security Logs (AC: 7)
- [ ] Setup security log segregation
  ```yaml
  # loki/security-logs.yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: loki-security-config
    namespace: monitoring
  data:
    config.yaml: |
      auth_enabled: true
      
      tenants:
        - name: security
          id: security-team
          limits:
            ingestion_rate_mb: 50
            ingestion_burst_size_mb: 100
            max_streams_per_user: 10000
            retention_period: 2160h  # 90 days
      
      schema_config:
        configs:
          - from: 2024-01-01
            store: boltdb-shipper
            object_store: azure
            schema: v12
            index:
              prefix: security_
              period: 24h
      
      storage_config:
        azure:
          container_name: security-logs
          account_name: ${SECURITY_STORAGE_ACCOUNT}
          account_key: ${SECURITY_STORAGE_KEY}
      
      ruler:
        alertmanager_url: http://security-alertmanager:9093
        evaluation_interval: 1m
        poll_interval: 1m
        rule_path: /rules/security
        storage:
          type: azure
          azure:
            container_name: security-rules
  ```
- [ ] Configure RBAC for logs
  ```javascript
  // auth/log-rbac.js
  class LogRBAC {
    constructor() {
      this.permissions = {
        'security-admin': {
          namespaces: ['*'],
          labels: ['*'],
          queries: ['*']
        },
        'security-viewer': {
          namespaces: ['security', 'audit'],
          labels: ['severity', 'source'],
          queries: ['read']
        },
        'developer': {
          namespaces: ['development', 'staging'],
          labels: ['app', 'version'],
          queries: ['read']
        },
        'operator': {
          namespaces: ['production'],
          labels: ['*'],
          queries: ['read']
        }
      };
    }
    
    validateQuery(user, query) {
      const role = user.role;
      const perms = this.permissions[role];
      
      if (!perms) return false;
      
      // Extract namespace from query
      const nsMatch = query.match(/namespace="([^"]+)"/);
      if (nsMatch) {
        const namespace = nsMatch[1];
        if (!perms.namespaces.includes('*') && 
            !perms.namespaces.includes(namespace)) {
          return false;
        }
      }
      
      return true;
    }
    
    filterResults(user, logs) {
      const role = user.role;
      const perms = this.permissions[role];
      
      return logs.filter(log => {
        if (perms.namespaces.includes('*')) return true;
        return perms.namespaces.includes(log.namespace);
      });
    }
  }
  ```
- [ ] Setup audit logging
- [ ] Configure compliance retention

### Task 8: Enable Real-time Streaming (AC: 8)
- [ ] Setup WebSocket streaming
  ```javascript
  // services/log-streaming.js
  const WebSocket = require('ws');
  const { PassThrough } = require('stream');
  
  class LogStreamingService {
    constructor(lokiClient) {
      this.lokiClient = lokiClient;
      this.connections = new Map();
    }
    
    setupWebSocketServer(server) {
      this.wss = new WebSocket.Server({ server, path: '/logs/stream' });
      
      this.wss.on('connection', (ws, req) => {
        const connectionId = this.generateConnectionId();
        const stream = new PassThrough();
        
        this.connections.set(connectionId, { ws, stream });
        
        ws.on('message', async (message) => {
          try {
            const request = JSON.parse(message);
            
            if (request.type === 'subscribe') {
              await this.handleSubscribe(connectionId, request);
            } else if (request.type === 'unsubscribe') {
              this.handleUnsubscribe(connectionId);
            }
          } catch (error) {
            ws.send(JSON.stringify({ error: error.message }));
          }
        });
        
        ws.on('close', () => {
          this.handleUnsubscribe(connectionId);
          this.connections.delete(connectionId);
        });
      });
    }
    
    async handleSubscribe(connectionId, request) {
      const { ws, stream } = this.connections.get(connectionId);
      const { query, start, delay } = request;
      
      // Start tailing logs
      const tailStream = await this.lokiClient.tailQuery({
        query,
        start: start || Date.now() - 3600000,
        delay_for: delay || 0,
        limit: 100
      });
      
      tailStream.on('data', (entry) => {
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({
            type: 'log',
            timestamp: entry.timestamp,
            line: entry.line,
            labels: entry.stream
          }));
        }
      });
      
      tailStream.on('error', (error) => {
        ws.send(JSON.stringify({
          type: 'error',
          message: error.message
        }));
      });
      
      this.connections.get(connectionId).tailStream = tailStream;
    }
    
    handleUnsubscribe(connectionId) {
      const connection = this.connections.get(connectionId);
      if (connection?.tailStream) {
        connection.tailStream.destroy();
      }
    }
    
    generateConnectionId() {
      return `conn_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
  }
  ```
- [ ] Configure streaming endpoints
- [ ] Setup rate limiting
- [ ] Test real-time delivery

---

## Dev Notes

### Loki Access
```
Query API: http://loki-gateway:3100
Push API: http://loki-distributor:3100
WebSocket: ws://logs.oversight.com/stream
```

### LogQL Examples
```logql
# Find errors in last hour
{job="oversight-app"} |= "error" | json | __error__=""

# Count requests by status
sum by (status) (
  rate({job="nginx"} | json | __error__="" [5m])
)

# Find slow database queries
{job="oversight-app"} 
  |= "Database Query" 
  | json 
  | duration > 1000

# Security events
{job="audit"} 
  |~ "unauthorized|forbidden|denied" 
  | json
```

### Performance Tuning
- Index high-cardinality labels carefully
- Use recording rules for complex queries
- Implement appropriate retention periods
- Optimize chunk size (512KB default)
- Configure appropriate cache sizes
- Use parallel queries for large time ranges

### Testing Standards
- Test location: `/tests/monitoring/logs/`
- Test log ingestion rate
- Verify query performance
- Test retention policies
- Monitor storage usage

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-22 | 1.0 | Initial story creation | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated during development_

### Completion Notes List
_To be populated during development_

### File List
_To be populated during development_

---

## QA Results
_To be populated by QA agent_